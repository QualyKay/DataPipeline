{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3aa260f-d3c9-40cf-9534-9ad9e41c24d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting pyspark\n  Using cached pyspark-3.5.0-py2.py3-none-any.whl\nCollecting py4j==0.10.9.7\n  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9.7 pyspark-3.5.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fbef65-d88c-4cb7-aa16-711d9705ae8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.datacohortworkspacelabs.dfs.core.windows.net\",\n",
    "    \"NUwhjFcHG95EU1Rnu7+Woq3JQP28bXy5kDQhA9yFV68XBz1umr7uqeQgMhrwxHfTwNWxAx/n1K6j+AStGTUMkQ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43759ba-db68-45f7-a4bf-8e81884fff11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp, input_file_name, expr\n",
    "\n",
    "landing_path = \"abfss://landing-kay@datacohortworkspacelabs.dfs.core.windows.net/yellow_tripdata_2019-*\"\n",
    "raw_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(landing_path)\n",
    "\n",
    "raw_data = raw_data.withColumn(\"tripID\", expr(\"uuid()\")) \\\n",
    "                             .withColumn(\"FileName\", input_file_name()) \\\n",
    "                             .withColumn(\"CreatedOn\", current_timestamp())\n",
    "\n",
    "bronze_path = \"abfss://bronze-kay@datacohortworkspacelabs.dfs.core.windows.net/\"\n",
    "raw_data.write.partitionBy(\"VendorID\").mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(bronze_path)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DB_1Bronze_Kay",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
